{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Retrieval Augmented Generation (RAG) System\n",
    "## dengan Chat Memory dan Session Management\n",
    "\n",
    "- **Author:** Sri Lutfiya Dwiyeni\n",
    "- **AIML 8**\n",
    "- **Topic:** Imagery Rehearsal Therapy (IRT) untuk Nightmare Disorder  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Penentuan Topik & Knowledge Base\n",
    "\n",
    "### Judul Topik\n",
    "**Efektivitas Imagery Rehearsal Therapy (IRT) dalam Mengurangi Nightmare Disorder, Gejala Depresi, Kecemasan, dan Ideasi Bunuh Diri pada Pasien Major Depressive Episode**\n",
    "\n",
    "### Ruang Lingkup Informasi\n",
    "Knowledge base ini mencakup:\n",
    "1. **Definisi dan Karakteristik Nightmare Disorder**\n",
    "   - Kriteria DSM-5-TR dan ICSD-3\n",
    "   - Prevalensi pada populasi umum vs psikiatrik\n",
    "\n",
    "2. **Hubungan Nightmare dengan Depresi dan Risiko Bunuh Diri**\n",
    "   - Mekanisme bagaimana mimpi buruk memprediksi perilaku bunuh diri\n",
    "   - Progresivitas: mimpi buruk → nightmare → skenario bunuh diri\n",
    "\n",
    "3. **Imagery Rehearsal Therapy (IRT)**\n",
    "   - Prinsip dasar berdasarkan Continuity Theory\n",
    "   - Protokol 4 sesi\n",
    "   - Mekanisme kerja: transformasi emosi dalam mimpi\n",
    "\n",
    "4. **Metodologi Penelitian**\n",
    "   - Desain studi non-randomized controlled\n",
    "   - Kriteria inklusi/eksklusi\n",
    "   - Instrumen pengukuran: NSI, QIDS-SR16, HAD, GAD-7\n",
    "\n",
    "5. **Hasil Klinis**\n",
    "   - Perbandingan efektivitas IRT vs Sleep Education Therapy (SET)\n",
    "   - Effect sizes: Cohen's d = 1.33 untuk NSI, 0.95 untuk ideasi bunuh diri\n",
    "   - Responder rates\n",
    "\n",
    "6. **Prediksi Respons Terapi**\n",
    "   - Faktor-faktor yang memprediksi keberhasilan IRT\n",
    "   - Treatment-resistant depression sebagai prediktor positif\n",
    "\n",
    "7. **Implikasi Klinis**\n",
    "   - Integrasi IRT dalam treatment plan\n",
    "   - Pencegahan eskalasi gejala depresi menjadi krisis bunuh diri\n",
    "\n",
    "### Alasan Pemilihan Topik\n",
    "1. **Prevalensi Tinggi**: Nightmare Disorder mempengaruhi 16.7% pasien dengan Major Depressive Episode (MDE), meningkat hingga 90% pada MDE dengan fitur melankolis atau perilaku bunuh diri\n",
    "\n",
    "2. **Prediktor Kuat Krisis Bunuh Diri**: 80% individu mengalami perubahan konten mimpi sebelum krisis bunuh diri\n",
    "\n",
    "3. **First-line Treatment**: IRT adalah rekomendasi internasional untuk Nightmare Disorder\n",
    "\n",
    "4. **Studi Penting**: First controlled study yang mengevaluasi IRT pada pasien MDE dengan Nightmare Disorder\n",
    "\n",
    "5. **Effect Size Besar**: Cohen's d = 1.33 untuk NSI menunjukkan efektivitas yang sangat tinggi\n",
    "\n",
    "6. **Temuan Mengejutkan**: Treatment-resistant depression memprediksi respons LEBIH BAIK terhadap IRT, membuka perspektif baru untuk populasi yang sulit diobati\n",
    "\n",
    "7. **Comprehensive Evaluation**: Mengukur 4 dimensi NSI (frequency, emotional impact, diurnal impact, nocturnal impact) plus depressive symptoms, anxiety, suicidal ideation, dan sleep quality\n",
    "\n",
    "8. **Early Intervention Potential**: Memahami progresivitas mimpi buruk dapat membantu pencegahan eskalasi ke krisis bunuh diri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Penyusunan Knowledge Base\n",
    "\n",
    "### Struktur Knowledge Base\n",
    "Knowledge base disusun dalam bentuk:\n",
    "1. **Source Documents**: File PDF dari jurnal penelitian\n",
    "2. **Document Chunks**: Text splits dengan overlap untuk context continuity\n",
    "3. **Vector Embeddings**: Representasi semantik dari setiap chunk\n",
    "4. **FAISS Index**: Vector database untuk efficient similarity search\n",
    "\n",
    "### Mekanisme Update Knowledge Base\n",
    "```python\n",
    "def update_knowledge_base(pdf_directory: str):\n",
    "    \"\"\"\n",
    "    Step 1: Load PDF documents from directory\n",
    "    Step 2: Split documents into chunks (size=1000, overlap=200)\n",
    "    Step 3: Generate embeddings for each chunk\n",
    "    Step 4: Create/overwrite FAISS vector store\n",
    "    Step 5: Recreate RAG chain with new vectorstore\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "**Keuntungan Struktur Ini:**\n",
    "- **Easy Addition**: Tinggal tambah PDF ke folder, lalu update\n",
    "- **Version Control**: Bisa rollback dengan restore old FAISS index\n",
    "- **Modular**: Setiap PDF diproses independently\n",
    "- **Scalable**: Support incremental indexing untuk dataset besar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Desain Sistem RAG\n",
    "\n",
    "### Komponen Utama\n",
    "\n",
    "#### 3.1. Large Language Model (LLM)\n",
    "\n",
    "**Model:** Groq Llama 3.3 70B Versatile\n",
    "\n",
    "**Alasan Pemilihan:**\n",
    "1. **Ultra-Fast Inference**: Groq menyediakan >500 tokens/second, crucial untuk chat experience\n",
    "2. **Large Context Window**: Support >8K tokens untuk long conversations\n",
    "3. **Strong Reasoning**: 70B parameters untuk pemahaman medical/clinical context\n",
    "4. **Multilingual**: Excellent support untuk Bahasa Indonesia\n",
    "5. **Cost-Effective**: Free tier dengan generous limits\n",
    "\n",
    "**Kelebihan:**\n",
    "- Response time < 2 seconds untuk complex answers\n",
    "- Konsisten dalam medical domain\n",
    "- Dapat handle nuanced clinical questions\n",
    "- Good safety guardrails untuk sensitive topics\n",
    "\n",
    "**Kesesuaian dengan Topik:**\n",
    "- Mampu memahami kompleksitas istilah psikologi/psikiatri\n",
    "- Sensitif terhadap konteks klinis (suicide ideation, depression)\n",
    "- Dapat explain technical concepts dalam bahasa awam\n",
    "- Support bilingual content (ID + EN medical terms)\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.2. Embedding Model\n",
    "\n",
    "**Model:** `sentence-transformers/paraphrase-multilingual-mpnet-base-v2`\n",
    "\n",
    "**Alasan Pemilihan:**\n",
    "1. **Multilingual Support**: Optimal untuk dokumen campuran ID + EN\n",
    "2. **MPNet Architecture**: Superior semantic understanding vs BERT\n",
    "3. **Paraphrase Detection**: Cocok untuk mencari dokumen dengan makna serupa\n",
    "4. **Balanced Size**: 278M parameters, efficient untuk CPU\n",
    "5. **Normalized Embeddings**: Better cosine similarity results\n",
    "\n",
    "**Spesifikasi Teknis:**\n",
    "- Embedding Dimension: 768\n",
    "- Max Sequence Length: 128 tokens\n",
    "- Model Size: ~420 MB\n",
    "- Inference Speed: ~50ms per chunk (CPU)\n",
    "\n",
    "**Kelebihan:**\n",
    "- High F1 score untuk semantic similarity tasks\n",
    "- Robust terhadap paraphrasing\n",
    "- Support 50+ languages\n",
    "- Maintained oleh Hugging Face\n",
    "\n",
    "**Kesesuaian dengan Topik:**\n",
    "- Tangkap sinonim medical terms (e.g., \"mimpi buruk\" = \"nightmare\")\n",
    "- Understand semantic similarity di mixed language text\n",
    "- Effective untuk clinical documentation\n",
    "- Preserve context dalam technical explanations\n",
    "\n",
    "---\n",
    "\n",
    "#### 3.3. Vector Database\n",
    "\n",
    "**Database:** FAISS (Facebook AI Similarity Search)\n",
    "\n",
    "**Alasan Pemilihan:**\n",
    "1. **Ultra-Fast Search**: Optimized C++ implementation\n",
    "2. **Memory Efficient**: IndexFlatL2 untuk small-medium datasets\n",
    "3. **No Server Overhead**: Embedded database, no network latency\n",
    "4. **Easy Persistence**: Save/load index dari disk\n",
    "5. **Production-Ready**: Used by Meta in production\n",
    "\n",
    "**Konfigurasi:**\n",
    "```python\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}  # Top-5 relevant chunks\n",
    ")\n",
    "```\n",
    "\n",
    "**Kelebihan:**\n",
    "- Exact L2 distance search (no approximation)\n",
    "- Sub-millisecond query time untuk small datasets\n",
    "- Support advanced indexing (IVF, HNSW) untuk scalability\n",
    "- Open source dengan active community\n",
    "\n",
    "**Kesesuaian dengan Topik:**\n",
    "- Perfect untuk 11-page PDF (manageable size)\n",
    "- Real-time retrieval untuk interactive chat\n",
    "- Extensible untuk add more psychology papers\n",
    "- Stable index untuk consistent knowledge base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Implementasi Aplikasi RAG\n",
    "\n",
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import uuid\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "# Suppress warnings\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Groq LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Groq LLM\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not GROQ_API_KEY:\n",
    "    raise ValueError(\" GROQ_API_KEY tidak ditemukan di environment!\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    groq_api_key=GROQ_API_KEY,\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0.3,  # Lower temperature untuk factual responses\n",
    "    max_tokens=2048\n",
    ")\n",
    "\n",
    "print(\" Groq LLM initialized\")\n",
    "print(f\"   Model: {llm.model}\")\n",
    "print(f\"   Temperature: {llm.temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HuggingFace Embeddings\n",
    "print(\" Loading embedding model...\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(\" Embedding model loaded\")\n",
    "print(f\"   Model: paraphrase-multilingual-mpnet-base-v2\")\n",
    "print(f\"   Embedding Dimension: 768\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Process PDF Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF documents\n",
    "pdf_directory = \"dataset/Psikologi\"\n",
    "\n",
    "print(f\" Loading PDFs from: {pdf_directory}\")\n",
    "\n",
    "documents = []\n",
    "pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith('.pdf')]\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(pdf_directory, pdf_file)\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    documents.extend(docs)\n",
    "    print(f\"  ✓ {pdf_file}: {len(docs)} pages\")\n",
    "\n",
    "print(f\"\\n Total documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Documents into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,        # Each chunk max 1000 characters\n",
    "    chunk_overlap=200,      # 200 characters overlap untuk context continuity\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Priority: paragraphs > sentences > words\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\" Document splitting completed\")\n",
    "print(f\"   Total chunks: {len(split_docs)}\")\n",
    "print(f\"   Chunk size: {text_splitter.chunk_size} chars\")\n",
    "print(f\"   Chunk overlap: {text_splitter.chunk_overlap} chars\")\n",
    "\n",
    "# Show sample chunk\n",
    "print(f\"\\n Sample chunk:\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(split_docs[0].page_content[:300] + \"...\")\n",
    "print(f\"{'=' * 80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create FAISS Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS vector store\n",
    "print(\" Creating FAISS vector store...\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(\" Vector store created\")\n",
    "print(f\"   Index type: FAISS IndexFlatL2\")\n",
    "print(f\"   Total vectors: {vectorstore.index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever from vector store\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}  # Retrieve top-5 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\" Retriever configured\")\n",
    "print(f\"   Search type: Similarity (L2 distance)\")\n",
    "print(f\"   Top-K: 5 chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Chat Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize chat message histories storage\n",
    "chat_histories = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
    "    \"\"\"\n",
    "    Get or create chat history for a session\n",
    "    \n",
    "    Args:\n",
    "        session_id: Unique identifier untuk session\n",
    "    \n",
    "    Returns:\n",
    "        ChatMessageHistory object untuk session tersebut\n",
    "    \"\"\"\n",
    "    if session_id not in chat_histories:\n",
    "        chat_histories[session_id] = ChatMessageHistory()\n",
    "    return chat_histories[session_id]\n",
    "\n",
    "print(\" Chat memory system initialized\")\n",
    "print(f\"   Storage: In-memory dictionary\")\n",
    "print(f\"   Support: Multiple independent sessions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RAG Chain with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt template with memory placeholder\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"Anda adalah asisten AI yang ahli dalam Imagery Rehearsal Therapy (IRT) untuk Nightmare Disorder.\n",
    "    \n",
    "Gunakan konteks berikut untuk menjawab pertanyaan:\n",
    "{context}\n",
    "\n",
    "Pedoman menjawab:\n",
    "1. Jawab dalam bahasa Indonesia yang jelas dan profesional\n",
    "2. Berikan jawaban yang akurat berdasarkan konteks yang diberikan\n",
    "3. Jika informasi tidak tersedia dalam konteks, katakan dengan jujur\n",
    "4. Gunakan istilah medis dengan penjelasan yang mudah dipahami\n",
    "5. Pertimbangkan riwayat percakapan sebelumnya untuk konteks yang lebih baik\"\"\"\n",
    ")\n",
    "\n",
    "# Messages placeholder for chat history\n",
    "history_placeholder = MessagesPlaceholder(variable_name=\"history\")\n",
    "\n",
    "# Human message template\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "\n",
    "# Combine into chat prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,\n",
    "    history_placeholder,\n",
    "    human_prompt\n",
    "])\n",
    "\n",
    "# Format documents function\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([f\"[Document {i+1}]\\n{doc.page_content}\" for i, doc in enumerate(docs)])\n",
    "\n",
    "# Create base RAG chain\n",
    "base_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | chat_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Wrap with message history\n",
    "rag_chain = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "print(\" RAG chain with memory created\")\n",
    "print(f\"   Components: Retriever → Prompt → LLM → Parser\")\n",
    "print(f\"   Memory: Session-aware chat history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test session\n",
    "test_session_id = str(uuid.uuid4())\n",
    "\n",
    "print(f\" Testing RAG system\")\n",
    "print(f\"Session ID: {test_session_id}\")\n",
    "print(f\"{'=' * 80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 1: Basic factual question\n",
    "question1 = \"Apa itu Imagery Rehearsal Therapy?\"\n",
    "\n",
    "print(f\"Q1: {question1}\")\n",
    "print(f\"{'-' * 80}\")\n",
    "\n",
    "response1 = rag_chain.invoke(\n",
    "    {\"question\": question1},\n",
    "    config={\"configurable\": {\"session_id\": test_session_id}}\n",
    ")\n",
    "\n",
    "print(f\"A1: {response1}\")\n",
    "print(f\"{'=' * 80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 2: Specific detail question\n",
    "question2 = \"Berapa sesi protokol IRT yang digunakan dalam penelitian?\"\n",
    "\n",
    "print(f\"Q2: {question2}\")\n",
    "print(f\"{'-' * 80}\")\n",
    "\n",
    "response2 = rag_chain.invoke(\n",
    "    {\"question\": question2},\n",
    "    config={\"configurable\": {\"session_id\": test_session_id}}\n",
    ")\n",
    "\n",
    "print(f\"A2: {response2}\")\n",
    "print(f\"{'=' * 80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 3: Comparative question\n",
    "question3 = \"Apa perbedaan efektivitas IRT vs Sleep Education Therapy?\"\n",
    "\n",
    "print(f\"Q3: {question3}\")\n",
    "print(f\"{'-' * 80}\")\n",
    "\n",
    "response3 = rag_chain.invoke(\n",
    "    {\"question\": question3},\n",
    "    config={\"configurable\": {\"session_id\": test_session_id}}\n",
    ")\n",
    "\n",
    "print(f\"A3: {response3}\")\n",
    "print(f\"{'=' * 80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 4: Testing memory - follow-up question\n",
    "question4 = \"Jelaskan tentang Nightmare Severity Index\"\n",
    "\n",
    "print(f\"Q4: {question4}\")\n",
    "print(f\"{'-' * 80}\")\n",
    "\n",
    "response4 = rag_chain.invoke(\n",
    "    {\"question\": question4},\n",
    "    config={\"configurable\": {\"session_id\": test_session_id}}\n",
    ")\n",
    "\n",
    "print(f\"A4: {response4}\")\n",
    "print(f\"{'=' * 80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Question 5: Testing memory - reference to previous context\n",
    "question5 = \"Berapa dimensi yang diukur?\"\n",
    "\n",
    "print(f\"Q5: {question5}\")\n",
    "print(f\"{'-' * 80}\")\n",
    "print(f\"[This tests if system remembers NSI from Q4]\\n\")\n",
    "\n",
    "response5 = rag_chain.invoke(\n",
    "    {\"question\": question5},\n",
    "    config={\"configurable\": {\"session_id\": test_session_id}}\n",
    ")\n",
    "\n",
    "print(f\"A5: {response5}\")\n",
    "print(f\"{'=' * 80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check chat history\n",
    "history = get_session_history(test_session_id)\n",
    "print(f\" Chat History Summary\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"Total messages: {len(history.messages)}\")\n",
    "print(f\"User messages: {len([m for m in history.messages if m.type == 'human'])}\")\n",
    "print(f\"AI messages: {len([m for m in history.messages if m.type == 'ai'])}\")\n",
    "print(f\"\\n Memory system working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Evaluasi & Analisis Sistem\n",
    "\n",
    "### Test Results Summary\n",
    "\n",
    "Dari testing di atas, sistem berhasil menjawab berbagai jenis pertanyaan dengan kualitas tinggi.\n",
    "\n",
    "**Kelebihan Sistem:**\n",
    "1.  Retrieval presisi untuk pertanyaan spesifik\n",
    "2.  Chat memory berfungsi sempurna untuk follow-up questions\n",
    "3.  Jawaban dalam Bahasa Indonesia yang profesional\n",
    "4.  Mengutip data statistik dengan akurat\n",
    "5.  Context-aware responses berdasarkan conversation history\n",
    "\n",
    "**Keterbatasan:**\n",
    "1.  Untuk pertanyaan sangat umum, retrieval mungkin terlalu narrow\n",
    "2.  Tidak ada source citation (user tidak tahu dari chunk mana jawaban berasal)\n",
    "3.  Long conversations bisa hit context window limits\n",
    "4.  Potential hallucination jika retrieved context tidak sufficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Insight & Recommendations\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Chunking Strategy = Foundation**: Optimal chunk size (1000) + overlap (200) more important than LLM model size\n",
    "2. **Memory Transforms UX**: Session-aware chat history mengubah RAG dari \"search\" ke \"conversation\"\n",
    "3. **Architecture Must Fit Context**: Speed > capability untuk chat, multilingual > pure performance untuk mixed-language content\n",
    "\n",
    "### Recommended Enhancements\n",
    "\n",
    "**Short-term:**\n",
    "- Add source citations dengan page numbers\n",
    "- Implement reranking untuk better precision\n",
    "- Add conversation summarization untuk long chats\n",
    "\n",
    "**Medium-term:**\n",
    "- Hybrid search (dense + sparse/BM25)\n",
    "- Metadata filtering untuk more targeted retrieval\n",
    "- Query expansion untuk better coverage\n",
    "\n",
    "**Long-term:**\n",
    "- Multi-modal RAG (process images/charts dari PDFs)\n",
    "- Active learning dengan user feedback\n",
    "- Agent-based RAG untuk complex reasoning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Personal Reflection\n",
    "\n",
    "### Reflection 1: Kualitas Knowledge Base\n",
    "\n",
    "Retrieval adalah jantung RAG. Pengalaman chunking (500 → 1000 chars) memberikan improvement lebih besar dari LLM upgrade (8B → 70B). \"Garbage in, garbage out\" sangat berlaku. Sekarang saya always start dengan: \"How should I structure KB for optimal retrieval?\" sebelum mikirin LLM.\n",
    "\n",
    "### Reflection 2: Context-Driven Architecture\n",
    "\n",
    "Optimal stack bukan \"best tools\" tapi \"best fit\". Groq's speed > GPT-4's capability untuk chat. MPNet multilingual > pure English untuk mixed-language. FAISS simplicity > Pinecone scalability untuk small dataset. No universal \"best\", only context-appropriate choices.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
